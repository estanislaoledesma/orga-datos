\documentclass[a4paper,10pt]{article}
\usepackage{pdfpages}
\usepackage{listings}
\usepackage{graphicx}
\usepackage[ansinew]{inputenc}
\usepackage[spanish]{babel}

\title{	\textbf{Dise\~{n}o del TP: Fine Foods Review} }

\author{
    Mart\'{i}n Queija, \textit{Padr\'{o}n Nro. 96.455} \\
	\texttt{ tinqueija@gmail.com } \\[2.5ex]
	Estanislao Ledesma, \textit{Padr\'{o}n Nro. 96.622} \\
	\texttt{ estanislaoledesma@gmail.com } \\[2.5ex]
	Mart\'{i}n Bosch, \textit{Padr\'{o}n Nro. 96.749} \\
	\texttt{ martinbosch17@gmail.com } \\[2.5ex]
	\normalsize{2do. Cuatrimestre de 2016} \\
	\normalsize{ Organizaci\'{o}n de Datos } \\
	\normalsize{ Facultad de Ingenier\'{i}a, Universidad de Buenos Aires } \\
}

\date{}

\begin{document}

	\maketitle
	\thispagestyle{empty}   % quita el n\'{u}mero en la primer p\'{a}gina


	\begin{abstract}
		\centerline{Aproximaci\'{o}n por regresi\'{o}n}
		
	\end{abstract}
	\newpage
	
	\tableofcontents
	
	
	\section{Introducci\'{o}n}
	
	El objetivo de este trabajo pr\'{a}ctico es predecir la puntuaci\'{o}n de distintas rese\~{n}as de comidas.
	
	\section{Dise\~{n}o}
	
	Para alcanzar el objetivo, la intenci\'{o}n es dise\~{n}ar un algoritmo que \textit{aprenda} a partir de un set de rese\~{n}as ya puntuadas para as\'{i} poder predecir la calificaci\'{o}n de nuevas rese\~{n}as. Parte del set de rese\~{n}as ya puntuadas se reserva para probar el "aprendisaje" del algortmo. De esta manera se puede determinar la presicion del algor\'{i}tmo al predecir rese\~{n}as y comparar la predicci\'{o}n con el valor esperado. La comparacion utilizada se denomina Mean Squared Error y se calcula sumatoria total de los errores al cuadrado sobre el total de puntos. Esta relaci\'{o}n proporciona un grado de confianza sobre nuestro algor\'{i}tmo para predecir puntuaciones de nuevas rese\~{n}as.
	
	
	\subsection{Preprocesamiento de los datos}	
	En una primera instancia es recomendable extraer toda la puntuaci\'{o}n de los datos crudos para no desperdiciar \textit{features} con s\'{i}mbolos que a la escencia de la implementaci\'{o}n no hacen de utilidad.
	
	\subsection{KNN}
	La base del dise\~{n}o se basa en el m\'{e}todo de reconocimiento de patrones llamado KNN (K-Nearest-Neighbour). Este m\'{e}todo es aplicable para realizar regresi\'{o}n o clasificaci\'{o}n. El set de datos provisto califica las rese\~{n}as con enteros del 1 al 5. Sin embargo el enunciado afirma que nuestro algort\'{i}mo puede predecir valores no enteros en el intervalo [1,5]. Por lo tanto establecemos que nuestro m\'{e}todo de predicci\'{o}n sera regresivo.
	
	El mecanismo de KNN consiste en encontrar los K-Vecinos mas cercanos a la nueva instancia de datos que queremos clasificar. Para clasificaci\'{o}n se predice la clase mayoritaria entre los K-Vecinos. Para regresi\'{o}n se calcula el promedio.
	
	El hyperpar\'{a}metro K se deduce a partir de una grid search, es decir, a partir de prueba y error probaremos diferentes valores de K para encontrar el que mejor ajuste a nuestro set de datos.
	
	\subsection{Apache Spark}
	El manejo de datos ser\'{a} responsabilidad de Spark. Apache Spark es un framework para implementar \textit{cluster computing.} Spark nos permite manipular el set de datos paralelamente de manera eficiente. 
	
	\subsection{BOW (Bag of words)}
	El enunciado del trabajo pr\'{a}ctico indica que es posible realizar una regresi\'{o}n con un alto grado de efectividad utilizando unicamente los campos de texto (Resumen y descripci\'{o}n) que compone cada rese\~{n}a. Procesar textos escritos por humanos contemplando la sintaxis, gramatica y puntuaci\'{o}n propone un problema de alta complejidad. A\'{u}n con un procesador de texto que contemple estas caracter\'{i}sticas de manera acertada existen muchas situaciones en las que ciertas combinaciones de palabras pueden tener doble significado o transmitir valor emocional que resulta casi imposible de parametrizar.
	
	Bag of words es una implementaci\'{o}n para pasar datos de texto a valores num\'{e}ricos (vectores). Se utiliza un diccionario con todas las posibles palabras que puedan aparecer y cada linea de texto se vectoriza notando la frecuencia de las palabras que contiene con respecto al diccionario. Esta aproximaci\'{o}n ignora el orden de las palabras, todo tipo de gram\'{a}tica y la sintaxis. Pero utilizar este m\'{e}todo tratando cada palabra como una \'{u}nica \textit{feature} propone que no se van a diferenciar casos tales como "La comida no era buena" de "La comida era buena".
	
	\subsection{N-Grams (Bag of N-Grams)}
	Aqu\'{i} es donde entra el papel de los n-gramas, este modelo propone vectorizar las \textit{features} pero contemplando una determinadad cantidad de palabras N de contexto. Esta implementaci\'{o}n nos permite diferenciar los casos que la simple implementaci\'{o}n de BOW no permite.
	El texto: "La comida no era buena" se parametriza de la sigueinte manera: \{"La comida no", "comida no era", "no era buena"\}. Luego, se vectoriza ubicando las frecuencias de las ocurrencias.
	
	\subsection{El problema de la dimensionalidad}
	Los N-gramas tienen una gran influencia en la presici\'{o}n de las predicciones. Utilizar N-gramas de mayor grado proporciona mejor interpretaci\'{o}n de las \textit{Bags of Words} a costas de incrementar de manera exponencial la dimension de los vectores resultantes. Esto se traduce directamente en un alto costo computacional. Estos vectores son altamente dispersos, en otras palabras, se almacena mucha informaci\'{o}n prescindible.
	
	\subsection{Diccionarios}
	La implementaci\'{o}n de los N-Gramas recientemente mencionada propone una primera pasada al set de datos para generar el vector con todas las distintas ocurrencias presentes. Esta implementaci\'{o}n requiere de una primera pasada de preproceso para armar el diccionario y luego cada dato genera su vector buscando los indicies que contienen las palabras que lo componen en el diccionario. Esta soluci\'{o}n propone busquedas en arreglos: aunque hoy en dia existen \'{o}ptimos metodos para ordenar arreglos y realizar b\'{u}squedas eficientes, la capacidad computacional que requiere es muy superior a utilizar funciones de hash.
	
	El \textit{Feature Vector} se puede obtener aplicando la tecnica llamada \textit{Feature Hashing}, donde cada feature del dato obtiene su indice de representaci\'{o}n en el \textit{Feature Vector} a partir de una funci\'{o}n de hash. Vel\'{o}z, simple y eficiente.
	
	En consecuencia el \textit{Feature Vector} suele ser un vector de gran dimensionalidad. Combinado con la gran dimensionalidad que presentan los N-Gramas se consideran vectores computacionalmente costosos de manipular.
	
	\subsection{Reducci\'{o}n de la dimensionalidad}
	
	Existen diversos m\'{e}todos de reducci\'{o}n de dimensionalidad. Estos m\'{e}todos son conocidos y sirven porque mantienen las distancias entre los puntos. De esta manera, podemos decrementar de manera significativa la cantidad de dimensiones del \textit{Feature Vector}.
	
	
	\subsection{LSH}
	
	Para evitar calcular por fueza bruta las distancias al aplicar KNN, utilizaremos LSH. Entonces dado el texto que queremos clasificar aplicamos una funcion de hashing y accedemos al bucket de la tabla de hash se√±alado por la misma, medimos las distancias con respecto a los elementos en el bucket y nos quedamos con los mas cercanos. Para reducir los falsos positivos (afectan la performance) utilizaremos mas de una funcion de hash o minihash, supongamos r funciones, obteniendolas de las familias universales de Carter-Wegman o las clasicas como Jenkins. Y para reducir los falsos negativos (afectan la precision) agruparemos estos minihash en b grupos, es decir, que vamos a tener b tablas de hash con T cantidad de buckets cada una.
	 Los hiperparametros b, r y T seran determinados de la siguiente manera: primero se determinara el b mas grande posible en base a la memoria disponible, para asi tener mayor presicion. Luego determinaremos r y T utilizando una grid search teniendo en cuenta la performance del algoritmo.

	
	\subsection{Funciones de hashing}

	A la hora de seleccionar funciones de hashing que ser\'{a}n utilizadas, como en LSH, donde se debe usar una gran cantidad de las mismas, se opta por las familias universales de Carter-Wegman. Se elige una familia universal ya que posee una baja probabilidad de colisiones para distintos objetos hasheados, y un grado alto de libertad al poder elegir los par\'{a}metros de las mismas. Para lograr mejores resultados, y generar la menor cantidad de errores de similitud (falsos positivos y negativos), es probable que se utilice un n\'{u}mero alto de funciones de hashing, no solamente utilizando la familia de Carter-Wegman sino que tambi\'{e}n se opta por combinar con funciones cl\'{a}sicas como Jenkins, City Hash y FNV.
	Tambi\'{e}n es probable que, al utilizar N-gramas, se puede aprovechar el proceso de armado de los mismos para incorporar la modalidad de hashing deslizante y as\'{i} obtener una gama bastante amplia de hashes, para reducir el esfuerzo computacional de comparaci\'{o}n de textos al m\'{a}ximo.
	
	\begin{thebibliography}{99}
		\bibitem{INT06} LSH (Locality Sensitive Hashing, WikiPedia, https://en.wikipedia.org/wiki/Locality\-sensitive\_hashing
		\bibitem{INT06} Data Paralelism , WikiPedia, https://en.wikipedia.org/wiki/Data\_parallelism.
		\bibitem{INT06} KNN (K Vecinos mas cercanos), WikiPedia, https://es.wikipedia.org/wiki/K\-vecinos\_m\%C3\%A1s\_cercano
		\bibitem{INT06} Apache Spark, WikiPedia,https://en.wikipedia.org/wiki/Apache\_Spark
		\bibitem{INT06} Programacion neurolinguistica, https://es.wikipedia.org/wikiProgramaci\%C3\%B3n\_neuroling\%C3\%BC\%C3\%ADstica
		\bibitem{INT06} Apunte oficial de la materia, Luis Argerich

	\end{thebibliography}
	
\end{document}
