\documentclass[a4paper,10pt]{article}
\usepackage{pdfpages}
\usepackage{listings}
\usepackage{graphicx}
\usepackage[ansinew]{inputenc}
\usepackage[spanish]{babel}

\title{		\textbf{Diseño del TP: Fine Foods Review}}

\author{	Martín Queija, \textit{Padrón Nro. 96.455}                     \\
	\texttt{ tinqueija@gmail.com }                                              \\[2.5ex]
	Estanislao Ledesma, \textit{Padrón Nro. 96.622}                     \\
	\texttt{ estanislaoledesma@gmail.com }                                              \\[2.5ex]
	Martín Bosch, \textit{Padrón Nro. 00.000}                     \\
	\texttt{ agus.luques@hotmail.com }                                              \\[2.5ex]
	\normalsize{2do. Cuatrimestre de 2016}                                      \\
	\normalsize{Organización de Datos  }  \\
	\normalsize{Facultad de Ingeniería, Universidad de Buenos Aires}            \\
}
\date{}

\begin{document}
	
	\maketitle
	\thispagestyle{empty}   % quita el número en la primer página
	
	
	\begin{abstract}
		\centerline{Aproximación por regresión}
		
	\end{abstract}
	\newpage
	
	\tableofcontents
	
	
	\section{Introducción}
	
	El objetivo de este trabajo práctico es predecir la puntuación de distintas reseñas de comidas.
	
	\section{Diseño}
	
	Para alcanzar el objetivo, la intención es diseñar un algoritmo que \textit{aprenda} a partir de un set de reseñas ya puntuadas para así poder predecir la calificación de nuevas reseñas. Parte del set de reseñas ya puntuadas se reserva para probar el "aprendisaje" del algorítmo. De esta manera se puede determinar la presicion del algorítmo al predecir reseñas y comparar la predicción con el valor esperado. La comparacion utilizada se denomina Mean Squared Error y se calcula sumatoria total de los errores al cuadrado sobre el total de puntos. Esta relación proporciona un grado de confianza sobre nuestro algorítmo para predecir puntuaciones de nuevas reseñas.
	
	
	\subsection{Preprocesamiento de los datos}	
	En una primera instancia es recomendable extraer toda la puntuación de los datos crudos para no desperdiciar \textit{features} con símbolos que a la escencia de la implementación no hacen de utilidad.
	
	\subsection{KNN}
	La base del diseño se basa en el método de reconocimiento de patrones llamado KNN (K-Nearest-Neighbour). Este método es aplicable para realizar regresión o clasificación. El set de datos provisto califica las reseñas con enteros del 1 al 5. Sin embargo el enunciado afirma que nuestro algorítmo puede predecir valores no enteros en el intervalo [1,5]. Por lo tanto establecemos que nuestro método de predicción sera regresivo.
	
	El mecanismo de KNN consiste en encontrar los K-Vecinos mas cercanos a la nueva instancia de datos que queremos clasificar. Para clasificación se predice la clase mayoritaria entre los K-Vecinos. Para regresión se calcula el promedio.
	
	El hyperparámetro K se deduce a partir de prueba y error. Probaremos diferentes valores de K para encontrar el que mejor ajuste a nuestro set de datos.
	
	\subsection{Apache Spark}
	El manejo de datos será responsabilidad de Spark. Apache Spark es un framework para implementar \textit{cluster computing.} Spark nos permite manipular el set de datos paralelamente de manera eficiente. 
	
	\subsection{BOW (Bag of words)}
	El enunciado del trabajo práctico indica que es posible realizar una regresión con un alto grado de efectividad utilizando unicamente los campos de texto (Resumen y descripción) que compone cada reseña. Procesar textos escritos por humanos contemplando la sintaxis, grámatica y puntuación propone un problema de alta complejidad. Aún con un procesador de texto que contemple estas características de manera acertada existen muchas situaciones en las que ciertas combinaciones de palabras pueden tener doble significado o transmitir valor emocional que resulta casi imposible de parametrizar.
	
	Bag of words es una implementacion para pasar datos de texto a valores numericos (vectores). Se utiliza un diccionario con todas las posibles palabras que puedan aparecer y cada linea de texto se vectoriza notando la frecuencia de las palabras que contiene con respecto al diccionario. Esta aproximación ignora el orden de las palabras, todo tipo de gramatica y la sintaxis. Utilizar este metodo tratando cada palabra como una única \textit{feature} propone que no se van a diferenciar casos tales como "La comida no era buena" de "La comida era buena".
	
	\subsection{N-Grams (Bag of N-Grams)}
	Aquí es donde entra el papel de los n-gramas, este modelo propone vectorizar las \textit{features} pero contemplando una determinadad cantidad de palabras N de contexto. Esta implementación nos permite diferenciar los casos que la simple implementación de BOW no permite.
	El texto: "La comida no era buena" se parametriza de la sigueinte manera: \{"La comida no", "comida no era", "no era buena"\}. Luego, se vectoriza ubicando las frecuencias de las ocurrencias.
	
	\subsection{Diccionarios?}
	La implementación de los N-Gramas recientemente mencionada propone una primera pasada al set de datos para generar el vector con todas las distintas ocurrencias presentes. Esto genera un vector de \textit{features} de alta dimensionalidad ademas de requerir una pasada más de preproceso. Una solución es aplicar la técnica llamada \textit{Feature Hashing}. Esta técnica nos permite generar el \textit{Feature Vector} donde el índice de cada \textit{feature} lo indica la funcion de hashing. La dimension del \textit{Feature Vector} la establecemos en el espacio de salida de la función de hashing. Si la dimensión de salida de la función de hash es menor a la cantidad de ocurrencias distintas en el set de entrenamiento palabras o \textit{features} similares seran consideradas como la misma ocurrencia. Esto determina un hyper-parámetro para nuestro algorítmo: La dimension de los \textit{Feature Vectors.}
	
	
	\subsection{El problema de la dimensionalidad}
	Los N-gramas tienen una gran influencia en la presición de las predicciones. Utilizar N-gramas de mayor grado proporciona mejor interpretación de las \textit{Bags of Words} a costas de incrementar de manera exponencial la dimension de los vectores resultantes. Esto se traduce directamente en un alto costo computacional. Estos vectores son altamente dispersos, en otras palabras, se almacena mucha información prescindible.
	
	\subsection{Reducción de la dimensionalidad}
	La información prescindible son dimensiones de más que contiene el vector. Existen diversos métodos de reducción de dimensiones
	
	\section{Otro titulo que podamos poner}
	
	mas texto
	
	\section{Conclusiones}
	
	Nuestra conclusion
	
	\begin{thebibliography}{99}
		\bibitem{INT06} LSH (Locality Sensitive Hashing, WikiPedia, https://en.wikipedia.org/wiki/Locality-sensitive_hashing
		\bibitem{INT06} Data Paralelism , WikiPedia, https://en.wikipedia.org/wiki/Data_parallelism.
		\bibitem{INT06} KNN (K Vecinos mas cercanos), WikiPedia, https://es.wikipedia.org/wiki/K-vecinos_m%C3%A1s_cercano
		\bibitem{INT06} Apache Spark, WikiPedia,https://en.wikipedia.org/wiki/Apache_Spark
		\bibitem{INT06} Programacion neurolinguistica, https://es.wikipedia.org/wikiProgramaci%C3%B3n_neuroling%C3%BC%C3%ADstica
		\bibitem{INT06} Apunte oficial de la materia, Luis Argerich

	\end{thebibliography}
	
\end{document}
