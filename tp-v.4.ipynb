{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN POR LIDERES Y SEGUIDORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "sc.stop()\n",
    "\n",
    "\"\"\"\n",
    "try:\n",
    "    type(sc)\n",
    "except NameError:\n",
    "    sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\"\"\"\n",
    "\n",
    "conf = (SparkConf()\n",
    "        .setMaster(\"local[3]\")\n",
    "        .setAppName(\"TP Orga\")\n",
    "        .set(\"spark.executor.cores\", \"3\")        # numero de cores a utilizar\n",
    "#        .set(\"spark.executor.memory\", \"2g\")     # cada executor puede correr en maximo 2 task al mismo tiempo\n",
    "#        .set(\"spark.executor.instances\", \"6\")    # numero de executors\n",
    "        .set(\"spark.dynamicAllocation.enabled\", \"true\") # usar estas dos lineas en vez de executor.instances\n",
    "        .set(\"spark.shuffle.service.enabled\", \"true\")\n",
    "\n",
    "        .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "#        .set(\"spark.driver.maxResultSize\", \"2g\")\n",
    "#        .set(\"spark.cores.max\", \"2\") \n",
    "         .set(\"spark.default.parallelism\", \"3\") )\n",
    "\n",
    "\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "#sc = pyspark.SparkContext('local[2]')\n",
    "\n",
    "print sc._conf.getAll()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "stop_words = [\"a\", \"about\", \"above\", \"after\", \"all\", \"am\", \"an\", \"and\", \"any\",\\\n",
    "                \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\",\\\n",
    "              \"below\", \"tween\", \"bot\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\",\\\n",
    "              \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\",\\\n",
    "              \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\",\\\n",
    "              \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\",\\\n",
    "              \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"u'if\", \"in\", \"into\", \"is\",\\\n",
    "              \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\",\\\n",
    "              \"my\", \"myself\", \"of\", \"off\", \"on\", \"once\",\\\n",
    "              \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\",\\\n",
    "              \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\",\\\n",
    "              \"should\", \"so\", \"some\", \"such\", \"than\", \"that\",\"that's\",\\\n",
    "              \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\",\\\n",
    "              \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\",\\\n",
    "              \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\",\\\n",
    "             \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\",\\\n",
    "              \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\",\\\n",
    "              \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\",\\\n",
    "              \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\",\\\n",
    "              \"yourself\", \"yourselves\"]\n",
    "\n",
    "stop_words = set(stop_words)\n",
    "\n",
    "palabras_sentimiento_inverso = [\"not\", \"ain't\", \"bareley\", \"aren't\", \"can't\", \"couldn't\", \"didn't\", \"doesn't\",\\\n",
    "                                \"don't\", \"hadn't\", \"few\", \"hardly\", \"low\", \"merely\", \"neither\", \"never\",\\\n",
    "                                \"no\", \"nor\", \"nobody\", \"none\", \"nope\", \"nothing\", \"rarely\", \"seldom\",\\\n",
    "                                \"hasn't\", \"haven't\", \"isn't\", \"mustn't\", \"shan't\", \"shouldn't\", \"used\",\\\n",
    "                                \"wasn't\", \"weren't\", \"won't\", \"wouldn't\", \"zero\"]\n",
    "\n",
    "\n",
    "palabras_sentimiento_inverso = set(palabras_sentimiento_inverso)\n",
    "\n",
    "\n",
    "\n",
    "def quitar_reviews_repetidos(reviewsRDD):\n",
    "    reviews_sin_rep = reviewsRDD.map( lambda review: (review.Text, review) )\\\n",
    "                    .reduceByKey( lambda review1, review2: review1 )\n",
    "\n",
    "    reviews_sin_rep = reviews_sin_rep.map(lambda x: x[1])\n",
    "\n",
    "    return reviews_sin_rep\n",
    "\n",
    "def preprocesar(review):\n",
    "    texto = quitar_signos(review.Text)\n",
    "    resumen = quitar_signos(review.Summary)\n",
    "\n",
    "#    texto = texto.lower()\n",
    "#    resumen = resumen.lower()\n",
    "\n",
    "    texto_palabras = texto.split(\" \")\n",
    "    resumen_palabras = resumen.split(\" \")\n",
    "\n",
    "#    texto_palabras = texto_palabras + ngrams(texto_palabras, 2)\n",
    "#    resumen_palabras = resumen_palabras + ngrams(resumen_palabras, 2)\n",
    "\n",
    "    texto_palabras = quitar_stopWords(texto_palabras)\n",
    "    resumen_palabras = quitar_stopWords(resumen_palabras)\n",
    "\n",
    "    estructura = [review.Id, review.Prediction, resumen_palabras+texto_palabras]\n",
    "\n",
    "    return estructura\n",
    "\n",
    "\n",
    "def quitar_signos(texto):\n",
    "    text = \"\"\n",
    "    if texto is not None:\n",
    "        text = texto.replace(\"<br />\", \" \")\n",
    "        text = re.sub(\"[^a-zA-Z' ]\", ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def quitar_stopWords(texto):\n",
    "    text = []\n",
    "\n",
    "    for i in range( len(texto) ):\n",
    "        palabra = texto[i].lower()\n",
    "        if palabra not in stop_words and palabra != '':\n",
    "\n",
    "            if palabra in palabras_sentimiento_inverso and i+1 < len(texto):\n",
    "                palabra = palabra + \" \" + texto[i+1].lower()\n",
    "                text.append(palabra)\n",
    "\n",
    "            if i > 0 and texto[i-1].lower() not in palabras_sentimiento_inverso:\n",
    "                text.append(palabra)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def ngrams(texto, n):\n",
    "    #texto = texto.split(\" \")\n",
    "    lista_ngrams = []\n",
    "    for i in xrange(0, len(texto)-n-1):\n",
    "        lista_ngrams.append(\" \".join(texto[i:i+n]))\n",
    "\n",
    "    return lista_ngrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creacion RDD train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark_csv as pycsv\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlCtx=SQLContext(sc)\n",
    "num_partition = 3\n",
    "\n",
    "trainRDD = sc.textFile('data_train.csv', num_partition)\n",
    "#trainRDD = sc.textFile('train.csv', num_partition)\n",
    "\n",
    "train_dataframe = pycsv.csvToDataFrame(sqlCtx, trainRDD, parseDate=False)\n",
    "train = train_dataframe.rdd\n",
    "\n",
    "\n",
    "train_sin_rep = quitar_reviews_repetidos(train)\n",
    "\n",
    "train_preprocesado = train_sin_rep.map(preprocesar)\n",
    "\n",
    "for x in train_preprocesado.take(1):\n",
    "    for y in x:\n",
    "        print y\n",
    "\n",
    "print \"Count: \", train_preprocesado.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bow(review):\n",
    "    texto = review[2]\n",
    "    frec = {}\n",
    "    for word in texto:\n",
    "        if word in frec:\n",
    "            frec[word] += 1\n",
    "        else:\n",
    "            frec[word] = 1\n",
    "\n",
    "    review.append(frec)\n",
    "#    review[2] = frec\n",
    "    return review\n",
    "\n",
    "train_preprocesado = train_preprocesado.map(bow)\n",
    "\n",
    "print \"Count: \", train_preprocesado.count()\n",
    "\n",
    "#train_preprocesado = train_preprocesado.cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def frec_por_doc(review):\n",
    "    texto = set(review[2])\n",
    "    words = []\n",
    "    for word in texto:\n",
    "        words.append( (word, 1) )\n",
    "\n",
    "    return words\n",
    "\n",
    "positivas = train_preprocesado.filter( lambda review: review[1]==4 or review[1]==5 or review[1]==3)\n",
    "cant_pos = positivas.count()\n",
    "\n",
    "frec_por_doc_pos = positivas.flatMap(frec_por_doc).reduceByKey( lambda x,y: x+y )\n",
    "frec_por_doc_pos = frec_por_doc_pos.collect()\n",
    "\n",
    "frec_pos = {}\n",
    "\n",
    "for word in frec_por_doc_pos:\n",
    "    frec_pos[word[0]] = word[1]\n",
    "\n",
    "frec_pos = sc.broadcast(frec_pos)\n",
    "\n",
    "negativas = train_preprocesado.filter( lambda review: review[1]==1 or review[1]==2 or review[1]==3 )\n",
    "cant_neg = negativas.count()\n",
    "\n",
    "frec_por_doc_neg = negativas.flatMap(frec_por_doc).reduceByKey( lambda x,y: x+y )\n",
    "frec_por_doc_neg = frec_por_doc_neg.collect()\n",
    "\n",
    "frec_neg = {}\n",
    "\n",
    "for word in frec_por_doc_neg:\n",
    "    frec_neg[word[0]] = word[1]\n",
    "\n",
    "frec_neg = sc.broadcast(frec_neg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "def delta_tfidf(review_test, review_train, frec_pos, cant_pos, frec_neg, cant_neg, promedio_long_docs):\n",
    "#def delta_tfidf(review_test, review_train):\n",
    "    texto = review_test[2]\n",
    "    frec_word = review_train[3]\n",
    "\n",
    "    r = 0\n",
    "    k = 1.2\n",
    "    # NORMALIZAR\n",
    "    b = 0.75\n",
    "    long_doc = len(texto)\n",
    "    norm = 1 - b + b * (long_doc / promedio_long_docs)\n",
    "\n",
    "    for word in texto:\n",
    "        ## TF-IDF\n",
    "        tf = 0\n",
    "\n",
    "        if word in frec_word:\n",
    "            #BM25\n",
    "            tf = ( (k+1)*frec_word[word] ) / ( frec_word[word] + k*norm )\n",
    "\n",
    "        idf_pos = 0\n",
    "        if word in frec_pos.value:\n",
    "            Pt = frec_pos.value[word]\n",
    "#            idf_pos = math.log( 1 + (cant_pos+1) / Pt)\n",
    "            idf_pos = math.log( 1 + (cant_pos) / Pt, 2)\n",
    "\n",
    "\n",
    "        idf_neg = 0\n",
    "        if word in frec_neg.value:\n",
    "            Nt = frec_neg.value[word]\n",
    "#            idf_neg = math.log( 1 + (cant_neg+1) / Nt)\n",
    "            idf_neg = math.log( 1 + (cant_neg) / Nt, 2)\n",
    "\n",
    "\n",
    "        r += tf * ( idf_pos - idf_neg )\n",
    "\n",
    "#    return (abs(r), review_train[1])\n",
    "    return (float( abs(r) ), review_train[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizacion datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cant_reviews = train_preprocesado.count()\n",
    "\n",
    "train_long_docs = train_preprocesado.map( lambda review: (1, len(review[2])) ).reduceByKey( lambda x, y: x+y )\n",
    "long_docs = train_long_docs.collect()\n",
    "long_docs = long_docs[0][1]\n",
    "print \"long: \", long_docs\n",
    "\n",
    "promedio_long_docs = float(long_docs) / cant_reviews\n",
    "print \"promedio: \", promedio_long_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIDERES Y SEGUIDORES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lideres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "#cant_lideres = int(sqrt(cant_reviews))\n",
    "cant_lideres = int(sqrt(cant_reviews)/8)\n",
    "lideres_lista = train_preprocesado.takeSample(False, cant_lideres, seed = 10L)\n",
    "\n",
    "lideres = sc.broadcast(lideres_lista)\n",
    "\n",
    "print \"cant_leaders: \", len(lideres.value)\n",
    "print \" \"\n",
    "print \"lider: \", lideres.value[0]\n",
    "\n",
    "\n",
    "print \"Count: \", train_preprocesado.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seguidores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def obtener_lideres_seguidores(seguidor, lideres, frec_pos, cant_pos, frec_neg, cant_neg, promedio_long_docs):\n",
    "#    lider_de_seguidor = lideres.value[0]\n",
    "    lider_de_seguidor = lideres.value[0][0]\n",
    "\n",
    "    max_dist = -1\n",
    "\n",
    "    for lider in lideres.value:\n",
    "        dist = delta_tfidf(seguidor, lider, frec_pos, cant_pos, frec_neg, cant_neg, promedio_long_docs)\n",
    "        dist = dist[0]\n",
    "        if dist > max_dist:\n",
    "            max_dist = dist\n",
    "#            lider_de_seguidor = lider\n",
    "            lider_de_seguidor = lider[0]\n",
    "\n",
    "    return (lider_de_seguidor, seguidor)\n",
    "\n",
    "lideres_seguidores = train_preprocesado.map( lambda seguidor: obtener_lideres_seguidores(seguidor, lideres, frec_pos, cant_pos, frec_neg, cant_neg, promedio_long_docs) )\n",
    "\n",
    "lideres_seguidores = lideres_seguidores.cache()\n",
    "\n",
    "#for x in lideres_seguidores.take(2):\n",
    "#    print \"lider: \", x[0]\n",
    "#    print \" \"\n",
    "#    print \"seguidor: \", x[1]\n",
    "#    print \" \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "aRDD = sc.textFile(\"a.csv\", num_partition)\n",
    "a_dataframe = pycsv.csvToDataFrame(sqlCtx, aRDD, parseDate=False)\n",
    "a = a_dataframe.rdd\n",
    "\n",
    "a_sin_rep = quitar_reviews_repetidos(a)\n",
    "a_preprocesado = a_sin_rep.map(preprocesar)\n",
    "a_preprocesado = a_preprocesado.map(bow)\n",
    "\n",
    "print a_preprocesado.count()\n",
    "\n",
    "a_lideres_seguidores = a_preprocesado.map( lambda seguidor: obtener_lideres_seguidores(seguidor, lideres, frec_pos, cant_pos, frec_neg, cant_neg, promedio_long_docs) )\n",
    "\n",
    "print a_lideres_seguidores.count()\n",
    "#print a_lideres_seguidores.take(1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#print lideres_seguidores.getNumPartitions()\n",
    "\n",
    "print \"Count: \", lideres_seguidores.count()\n",
    "\n",
    "#print lideres_seguidores.map(lambda x: 1).reduce(lambda x,y: x+y)\n",
    "\n",
    "\"\"\"\n",
    "#seguidor = train_preprocesado.take(2)[1]\n",
    "seguidor = lideres.value[1]\n",
    "\n",
    "#lider = lideres.value[0]\n",
    "#dist = delta_tfidf(seguidor, lider, frec_pos, cant_pos, frec_neg, cant_neg, promedio_long_docs)\n",
    "#print \"dist: \", dist\n",
    "#print \" \"\n",
    "\n",
    "a = obtener_lideres_seguidores(seguidor, lideres, frec_pos, cant_pos, frec_neg, cant_neg, promedio_long_docs)\n",
    "\n",
    "#print \"a: \", a\n",
    "print \"lider: \", a[0]\n",
    "print \" \"\n",
    "print \"seguidor: \", a[1]\n",
    "print \" \"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "lider = 85734\n",
    "\n",
    "seguidores = lideres_seguidores.filter( lambda x: x[0][0]==lider )\n",
    "\n",
    "print seguidores.getNumPartitions()\n",
    "\n",
    "for x in seguidores.take(1):\n",
    "    print \"lider: \", x[0]\n",
    "    print \" \"\n",
    "    print \"seguidor: \", x[1]\n",
    "    print \" \"\n",
    "\n",
    "#print seguidores.count()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN con Lideres-Seguidores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lideresRDD = sc.parallelize(lideres_lista, num_partition)\n",
    "\n",
    "def obtener_dist_a_lideres(review_test, lider, frec_pos, cant_pos, frec_neg, cant_neg, promedio_long_docs):\n",
    "    dist = delta_tfidf(review_test, lider, frec_pos, cant_pos, frec_neg, cant_neg, promedio_long_docs)\n",
    "    dist = dist[0]\n",
    "\n",
    "    return (dist, lider)\n",
    "\n",
    "def obtener_lideres_mas_cercanos(review_test, cant_lideres, lideresRDD, frec_pos, cant_pos, frec_neg, cant_neg, promedio_long_docs):\n",
    "    dist_lider = lideresRDD.map(lambda lider: obtener_dist_a_lideres(review_test, lider, frec_pos, cant_pos, frec_neg, cant_neg, promedio_long_docs))\n",
    "\n",
    "    lideres_mas_cercanos = dist_lider.takeOrdered(cant_lideres, key=lambda x: -x[0])\n",
    "\n",
    "    lideres_cercanos = []\n",
    "    for lider in lideres_mas_cercanos:\n",
    "        lideres_cercanos.append(lider[1][0])\n",
    "\n",
    "    lideres_cercanos = set(lideres_cercanos)\n",
    "    return lideres_cercanos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn2(review_test, k, lideresRDD, lideres_cercanos, frec_pos, cant_pos, frec_neg, cant_neg, promedio_long_docs):\n",
    "    cant_lideres = 3\n",
    "    lideres_cercanos = obtener_lideres_mas_cercanos(review_test, cant_lideres, lideresRDD, frec_pos, cant_pos, frec_neg, cant_neg, promedio_long_docs)\n",
    "\n",
    "    seguidores = lideres_seguidores.filter( lambda lider: lider[0] in lideres_cercanos )\n",
    "\n",
    "    dist_seguidores = seguidores.map(lambda seguidor: delta_tfidf(review_test, seguidor[1], frec_pos, cant_pos, frec_neg, cant_neg, promedio_long_docs))\n",
    "\n",
    "    k_mas_cercanos = dist_seguidores.takeOrdered(k, key=lambda x: -x[0])\n",
    "\n",
    "#    k_mas_cercanos = lideres_seguidores.filter( lambda lider: lider[0] in lideres_cercanos )\\\n",
    "#                                        .map(lambda seguidor: delta_tfidf(review_test, seguidor[1], frec_pos, cant_pos, frec_neg, cant_neg, promedio_long_docs))\\\n",
    "#                                        .takeOrdered(k, key=lambda x: -x[0])\n",
    "\n",
    "    return k_mas_cercanos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predecir uno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## PREDECIR UNO SOLO\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "#import time\n",
    "\n",
    "def predecir(k_mas_cercanos):\n",
    "    prediccion_total = 0\n",
    "    for cercano in k_mas_cercanos:\n",
    "        prediccion_total += cercano[1]\n",
    "\n",
    "    return float(prediccion_total) / len(k_mas_cercanos)\n",
    "    ## REDONDEANDO\n",
    "#    return prediccion_total / len(k_mas_cercanos)\n",
    "\n",
    "\n",
    "reviewRDD = sc.textFile(\"review\")\n",
    "review_dataframe = pycsv.csvToDataFrame(sqlCtx, reviewRDD, parseDate=False)\n",
    "reviews = review_dataframe.rdd\n",
    "\n",
    "reviews_preprocesado = reviews.map(preprocesar).map(bow)\n",
    "test_pred = reviews_preprocesado.collect()\n",
    "test_pred = test_pred[0]\n",
    "\n",
    "print \"TEST_PRED: \", test_pred[0:2]\n",
    "\n",
    "cant_lideres = 1\n",
    "lideres_cercanos = obtener_lideres_mas_cercanos(test_pred, cant_lideres, lideresRDD, frec_pos, cant_pos, frec_neg, cant_neg, promedio_long_docs)\n",
    "\n",
    "print \"lideres_cercanos: \", lideres_cercanos\n",
    "\n",
    "k = 3\n",
    "\"\"\"\n",
    "k_mas_cercanos = lideres_seguidores.filter( lambda lider: lider[0] in lideres_cercanos )\\\n",
    "                                    .map(lambda seguidor: delta_tfidf(test_pred, seguidor[1], frec_pos, cant_pos, frec_neg, cant_neg, promedio_long_docs))\\\n",
    "                                    .takeOrdered(k, key=lambda x: -x[0])\n",
    "\"\"\"\n",
    "\n",
    "k_mas_cercanos = knn2(test_pred, k, lideresRDD, lideres_cercanos, frec_pos, cant_pos, frec_neg, cant_neg, promedio_long_docs)\n",
    "\n",
    "print k_mas_cercanos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predecir varios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## PREDECIR 10\n",
    "\n",
    "from math import sqrt\n",
    "from timeit import default_timer as timer\n",
    "#import time\n",
    "\n",
    "def predecir(k_mas_cercanos):\n",
    "    prediccion_total = 0\n",
    "    for cercano in k_mas_cercanos:\n",
    "        prediccion_total += cercano[1]\n",
    "\n",
    "    return float(prediccion_total) / len(k_mas_cercanos)\n",
    "    ## REDONDEANDO\n",
    "#    return prediccion_total / len(k_mas_cercanos)\n",
    "\n",
    "## 100\n",
    "testRDD = sc.textFile('data_test.csv', num_partition)\n",
    "test_dataframe = pycsv.csvToDataFrame(sqlCtx, testRDD, parseDate=False)\n",
    "test = test_dataframe.rdd\n",
    "test_sin_rep = quitar_reviews_repetidos(test)\n",
    "test_preprocesado = test_sin_rep.map(preprocesar)\n",
    "\n",
    "reviews = test_preprocesado.take(100)\n",
    "cant_ej = 100\n",
    "\n",
    "## 10\n",
    "\"\"\"\n",
    "reviewRDD = sc.textFile(\"reviews_test\")\n",
    "review_dataframe = pycsv.csvToDataFrame(sqlCtx, reviewRDD, parseDate=False)\n",
    "reviews = review_dataframe.rdd\n",
    "reviews_preprocesado = reviews.map(preprocesar)\n",
    "\n",
    "reviews = reviews_preprocesado.collect()\n",
    "cant_ej = 10\n",
    "\"\"\"\n",
    "\n",
    "#test = test_preprocesado.take(cant_ej)\n",
    "\n",
    "#for t in test:\n",
    "#    print \"t: \", t\n",
    "\n",
    "start = timer()\n",
    "\n",
    "## PREDECIR VARIOS\n",
    "error = 0\n",
    "correct = 0\n",
    "for test_pred in reviews:\n",
    "    print \"TEST_PRED: \", test_pred[0:2]\n",
    "\n",
    "    lideres_cercanos = obtener_lideres_mas_cercanos(test_pred, 1, lideresRDD, frec_pos, cant_pos, frec_neg, cant_neg, promedio_long_docs)\n",
    "\n",
    "    k_mas_cercanos = knn2(test_pred, 3, lideresRDD, lideres_cercanos, frec_pos, cant_pos, frec_neg, cant_neg, promedio_long_docs)\n",
    "    print \"k_mas_cercanos: \", k_mas_cercanos\n",
    "\n",
    "    prediccion = predecir(k_mas_cercanos)\n",
    "\n",
    "    print \"PRED: \", prediccion\n",
    "    print \" \"\n",
    "\n",
    "    neg = prediccion <= 3.5 and test_pred[1] <= 3.5\n",
    "    pos = prediccion > 3.5 and test_pred[1] > 3.5\n",
    "    if neg or pos:\n",
    "        correct += 1\n",
    "    error += (prediccion - test_pred[1])**2\n",
    "\n",
    "error = sqrt(error)\n",
    "print \"ERROR: \", error\n",
    "\n",
    "print \"accuracy: \", ( float(correct)/cant_ej ) * 100\n",
    "\n",
    "\n",
    "end = timer()\n",
    "print \"time: \", (end-start)\n",
    "\n",
    "#p = float(correct) / cant_ej\n",
    "#r = float(correct) / train_preprocesado.count()\n",
    "#print \"F1 score: \", ( 2*p*r / (p+r) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predecir test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocesar_test(review):\n",
    "    texto = quitar_signos(review.Text)\n",
    "    resumen = quitar_signos(review.Summary)\n",
    "\n",
    "#    texto = texto.lower()\n",
    "#    resumen = resumen.lower()\n",
    "\n",
    "    texto_palabras = texto.split(\" \")\n",
    "    resumen_palabras = resumen.split(\" \")\n",
    "\n",
    "#    texto_palabras = texto_palabras + ngrams(texto_palabras, 2)\n",
    "#    resumen_palabras = resumen_palabras + ngrams(resumen_palabras, 2)\n",
    "\n",
    "    texto_palabras = quitar_stopWords(texto_palabras)\n",
    "    resumen_palabras = quitar_stopWords(resumen_palabras)\n",
    "\n",
    "    estructura = [review.Id, -1, resumen_palabras+texto_palabras]\n",
    "\n",
    "    return estructura\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from math import sqrt\n",
    "from timeit import default_timer as timer\n",
    "#import time\n",
    "\n",
    "def predecir(k_mas_cercanos):\n",
    "    prediccion_total = 0\n",
    "    for cercano in k_mas_cercanos:\n",
    "        prediccion_total += cercano[1]\n",
    "\n",
    "    return float(prediccion_total) / len(k_mas_cercanos)\n",
    "\n",
    "\n",
    "reviewRDD = sc.textFile(\"test.csv\")\n",
    "review_dataframe = pycsv.csvToDataFrame(sqlCtx, reviewRDD, parseDate=False)\n",
    "reviews = review_dataframe.rdd\n",
    "\n",
    "reviews_preprocesado = reviews.map(preprocesar_test)\n",
    "reviews = reviews_preprocesado.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    predic = open(\"test_predicciones.txt\", 'w')\n",
    "except IOError as exc:\n",
    "    print \"Error al abrir el archivo de entrada, más información: \", exc\n",
    "\n",
    "start = timer()\n",
    "\n",
    "cant_ej = 0\n",
    "\n",
    "error = 0\n",
    "correct = 0\n",
    "for test_pred in reviews:\n",
    "    cant_ej += 1\n",
    "\n",
    "    lideres_cercanos = obtener_lideres_mas_cercanos(test_pred, 1, lideresRDD, frec_pos, cant_pos, frec_neg, cant_neg, promedio_long_docs)\n",
    "\n",
    "    k_mas_cercanos = knn2(test_pred, 3, lideresRDD, lideres_cercanos, frec_pos, cant_pos, frec_neg, cant_neg, promedio_long_docs)\n",
    "\n",
    "    prediccion = predecir(k_mas_cercanos)\n",
    "\n",
    "    neg = prediccion <= 3.5 and test_pred[1] <= 3.5\n",
    "    pos = prediccion > 3.5 and test_pred[1] > 3.5\n",
    "    if neg or pos:\n",
    "        correct += 1\n",
    "    error += (prediccion - test_pred[1])**2\n",
    "\n",
    "    if cant_ej % 10 == 0:\n",
    "        print \"TEST_PRED: \", test_pred[0:2]\n",
    "        print \"k_mas_cercanos: \", k_mas_cercanos\n",
    "        print \"PRED: \", prediccion\n",
    "        print \"cant_ej: \", cant_ej\n",
    "        print \" \"\n",
    "\n",
    "    predic.write(str(prediccion) + \" \" + str(test_pred[0]))\n",
    "\n",
    "\n",
    "predic.close()\n",
    "error = sqrt(error)\n",
    "print \"ERROR: \", error\n",
    "\n",
    "print \"accuracy: \", ( float(correct)/cant_ej ) * 100\n",
    "\n",
    "end = timer()\n",
    "print \"time: \", (end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
