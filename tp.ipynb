{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "try:\n",
    "    type(sc)\n",
    "except NameError:\n",
    "    sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reviewsRDD = reviewsRDD_.map(lambda review: review.split(\",\"))\n",
    "#reviewsRDD.take(2)\n",
    "\n",
    "#data.first()\n",
    "#test.take(2)\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "import re\n",
    "\n",
    "stop_words = get_stop_words('english')\n",
    "stop_words = set(stop_words)\n",
    "\n",
    "\n",
    "def quitar_reviews_repetidos(reviewsRDD):\n",
    "    reviews_sin_rep = reviewsRDD.map( lambda review: (review.Text, review) )\\\n",
    "                    .reduceByKey( lambda review1, review2: review1 )\n",
    "\n",
    "    reviews_sin_rep = reviews_sin_rep.map(lambda x: x[1])\n",
    "\n",
    "    return reviews_sin_rep\n",
    "\n",
    "def preprocesar(review):\n",
    "    texto = quitar_signos(review.Text)\n",
    "    resumen = quitar_signos(review.Summary)\n",
    "\n",
    "    texto_palabras = texto.split(\" \")\n",
    "    resumen_palabras = resumen.split(\" \")\n",
    "\n",
    "    # Antes de quitar las stopWords armo los ngramas, sino\n",
    "    # estos no tendrian sentido\n",
    "    ngrams_text = ngrams(texto_palabras, 3)\n",
    "    ngrams_resumen = ngrams(resumen_palabras, 3)\n",
    "\n",
    "    text = quitar_stopWords(texto_palabras)\n",
    "    resumen = quitar_stopWords(resumen_palabras)\n",
    "\n",
    "    estructura = [review.Id, review.Prediction, resumen+text+ngrams_text+ngrams_resumen]\n",
    "\n",
    "    return estructura\n",
    "\n",
    "\n",
    "def quitar_signos(texto):\n",
    "    #signos = ['(',')','<br />']\n",
    "    #signos = r'\\()\\<br />'\n",
    "    #return [review.Text.strip(signos)]\n",
    "    text = \"\"\n",
    "    if texto is not None:\n",
    "        text = texto.replace(\"<br />\", \" \")\n",
    "        text = re.sub(\"[^a-zA-Z' ]\", '', text)\n",
    "\n",
    "    #resumen = re.sub(\"[^a-zA-Z' ]\", ' ', review.Summary)\n",
    "    #'[\\(\\)\\{\\}<>]'\n",
    "    #text_palabras = text.split(\" \")\n",
    "    #resumen_palabras = resumen.split(\" \")\n",
    "    return text\n",
    "\n",
    "def ngrams(texto, n):\n",
    "    #texto = texto.split(\" \")\n",
    "    lista_ngrams = []\n",
    "    for i in xrange(0, len(texto)-n-1):\n",
    "        lista_ngrams.append(\" \".join(texto[i:i+n]))\n",
    "\n",
    "    return lista_ngrams\n",
    "\n",
    "\n",
    "def quitar_stopWords(texto):\n",
    "    text = []\n",
    "    for palabra in texto:\n",
    "        if palabra.lower() not in stop_words:\n",
    "            text.append(palabra)\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark_csv as pycsv\n",
    "\n",
    "#sc.addPyFile('/tmp/spark-3a633a6f-578e-41b5-a353-143f66408280/userFiles-dea1cdfe-5203-4304-9d70-0a839a83714b/pyspark_csv.py')\n",
    "\n",
    "trainRDD = sc.textFile('data_train.csv')\n",
    "train_dataframe = pycsv.csvToDataFrame(sqlCtx, trainRDD, parseDate=False)\n",
    "train = train_dataframe.rdd\n",
    "\n",
    "## CONTAR REVIEW REPETIDO\n",
    "#train_sin_rep = train.map( lambda texto: (texto.ProductId + texto.UserId, 1) )\\\n",
    "#                    .reduceByKey( lambda a,b: a+b )\n",
    "#print train_sin_rep.takeOrdered(10, lambda x: -x[1])\n",
    "\n",
    "## QUITAR REVIEW REPETIDO\n",
    "#train_sin_rep = train.map( lambda texto: (texto.ProductId + texto.UserId, texto) )\\\n",
    "#                    .reduceByKey( lambda texto1, texto2: texto1 )\n",
    "#train_sin_rep = train_sin_rep.map(lambda x: x[1])\n",
    "\n",
    "\n",
    "train_sin_rep = quitar_reviews_repetidos(train)\n",
    "\n",
    "# VER SI HAY REVIEW REPETIDO\n",
    "#a = train_sin_rep.map( lambda texto: (texto.Text, 1) )\\\n",
    "#                    .reduceByKey( lambda a,b: a+b )\n",
    "#print \"ordered:\", a.takeOrdered(10, lambda x: -x[1])\n",
    "\n",
    "\n",
    "train_preprocesado = train_sin_rep.map(preprocesar)\n",
    "\n",
    "#for x in train_preprocesado.take(1):\n",
    "#    for y in x:\n",
    "#        print y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201541\n",
      "3\n",
      "[u'Entirely', u'Satisfied', u'Even', u'though', u'shipping', u'super', u'fast', u'good', u'hoped', u'pretty', u'sure', u'received', u'pound', u'half', u'', u'definitely', u'full', u'two', u'pounds', u'', u'guess', u'fine', u'see', u'finishing', u'given', u'flavor', u'decent', u'near', u'good', u'dried', u'papaya', u'can', u'seem', u'fresh', u'either', u'probably', u'wont', u'reordering', u'item', u'Even though the', u'though the shipping', u'the shipping was', u'shipping was super', u'was super fast', u'super fast it', u'fast it was', u'it was not', u'was not as', u'not as good', u'as good as', u'good as I', u'as I hoped', u'I hoped it', u'hoped it would', u'it would be', u\"would be I'm\", u\"be I'm pretty\", u\"I'm pretty sure\", u'pretty sure I', u'sure I only', u'I only received', u'only received a', u'received a pound', u'a pound and', u'pound and a', u'and a half', u'a half ', u'half  definitely', u' definitely not', u'definitely not the', u'not the full', u'the full two', u'full two pounds', u'two pounds ', u'pounds  which', u' which I', u'which I guess', u'I guess is', u'guess is fine', u'is fine because', u'fine because I', u\"because I don't\", u\"I don't see\", u\"don't see myself\", u'see myself finishing', u'myself finishing what', u'finishing what I', u'what I was', u'I was given', u'was given The', u'given The flavor', u'The flavor itself', u'flavor itself was', u'itself was decent', u'was decent but', u'decent but no', u'but no where', u'no where near', u'where near as', u'near as good', u'as good as', u'good as dried', u'as dried papaya', u'dried papaya can', u'papaya can be', u'can be It', u\"be It didn't\", u\"It didn't seem\", u\"didn't seem all\", u'seem all that', u'all that fresh', u'that fresh either', u'fresh either I', u'either I probably', u'I probably wont', u'probably wont be', u'wont be reordering', u'be reordering this']\n"
     ]
    }
   ],
   "source": [
    "testRDD = sc.textFile('data_test.csv')\n",
    "test_dataframe = pycsv.csvToDataFrame(sqlCtx, testRDD, parseDate=False)\n",
    "test = test_dataframe.rdd\n",
    "\n",
    "test_sin_rep = quitar_reviews_repetidos(test)\n",
    "\n",
    "test_preprocesado = test_sin_rep.map(preprocesar)\n",
    "\n",
    "for x in test_preprocesado.take(1):\n",
    "    #print \"x: \", x\n",
    "    for y in x:\n",
    "        print y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_PRED:  [201541, 3]\n",
      "PRED:  3.0\n",
      "TEST_PRED:  [6911, 2]\n",
      "PRED:  3.8\n",
      "TEST_PRED:  [461853, 5]\n",
      "PRED:  4.6\n",
      "TEST_PRED:  [563294, 5]\n",
      "PRED:  5.0\n",
      "TEST_PRED:  [471402, 5]\n",
      "PRED:  4.6\n",
      "TEST_PRED:  [139618, 5]\n",
      "PRED:  4.8\n",
      "TEST_PRED:  [248946, 5]\n",
      "PRED:  4.9\n",
      "TEST_PRED:  [163663, 1]\n",
      "PRED:  3.8\n",
      "TEST_PRED:  [140792, 5]\n",
      "PRED:  4.9\n",
      "TEST_PRED:  [340744, 1]\n",
      "PRED:  2.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntest_pred = test[1]\\nprint \"test: \", test_pred\\nprint \" \"\\n\\nk_mas_cercanos = knn(test_pred)\\nprint k_mas_cercanos\\n\\nprediccion = predecir(k_mas_cercanos)\\n\\nprint \"PRED: \", prediccion\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def distancia(texto1_estruct, texto2_estruct):\n",
    "    texto1 = set(texto1_estruct[2])\n",
    "    texto2 = set(texto2_estruct[2])\n",
    "\n",
    "    interseccion = len(texto1.intersection(texto2))\n",
    "    union = len(texto1.union(texto2))\n",
    "\n",
    "    return [texto2_estruct[0], texto2_estruct[1], float(interseccion)/float(union)]\n",
    "\n",
    "\n",
    "def knn(texto_estruct):\n",
    "    k = 10\n",
    "\n",
    "    dist = train_preprocesado.map(lambda vecino: distancia(texto_estruct, vecino))\n",
    "    k_mas_cercanos = dist.takeOrdered(k, key=lambda x: -x[2])\n",
    "\n",
    "    return k_mas_cercanos\n",
    "\n",
    "def predecir(k_mas_cercanos):\n",
    "    prediccion_total = 0\n",
    "    for cercano in k_mas_cercanos:\n",
    "        prediccion_total += cercano[1]\n",
    "\n",
    "    return float(prediccion_total) / len(k_mas_cercanos)\n",
    "\n",
    "\n",
    "test = test_preprocesado.take(10)\n",
    "#for t in test:\n",
    "#    print \"t: \", t\n",
    "\n",
    "## PREDECIR VARIOS\n",
    "for test_pred in test:\n",
    "    print \"TEST_PRED: \", test_pred[0:2]\n",
    "    k_mas_cercanos = knn(test_pred)\n",
    "    prediccion = predecir(k_mas_cercanos)\n",
    "\n",
    "    print \"PRED: \", prediccion\n",
    "\n",
    "\n",
    "## PREDECIR UNO\n",
    "\"\"\"\n",
    "test_pred = test[1]\n",
    "print \"test: \", test_pred\n",
    "print \" \"\n",
    "\n",
    "k_mas_cercanos = knn(test_pred)\n",
    "print k_mas_cercanos\n",
    "\n",
    "prediccion = predecir(k_mas_cercanos)\n",
    "\n",
    "print \"PRED: \", prediccion\n",
    "\"\"\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
